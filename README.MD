# README

## Introduction

This repository contains the code used for my master's thesis, which focuses on **constructing a portfolio of optimization algorithms running in parallel** to combine their complementary strengths and achieve **optimized anytime performance** across a suite of numerical objective functions. 
The experiments involve running various optimization algorithms on multiple benchmark functions, collecting and analyzing their results, and finally developing a **portfolio approach** that improves the overall performance of the system.

This project leverages optimization frameworks like **Nevergrad** and **IOHanalyzer** to evaluate the performance of different algorithms on a wide range of standard benchmark functions, recording the results across multiple runs.

### Objective
The goal of the project is to:
1. Run various single algorithms across a suite of numerical objective functions.
2. Record their performance in terms of solution quality over a range of budgets.
3. Develop a portfolio of optimization algorithms that enhances overall performance based on the observed results.

---

## File Descriptions

### 1. `exp_single_algo.py`

This script runs a set of **single optimization algorithms** on a variety of **benchmark functions** and logs the results for analysis. 
The algorithms are evaluated based on their ability to optimize a series of objective functions from the **Nevergrad** library. 
The results are recorded using the **IOHanalyzer** tool for further analysis.

#### Key Functionality:
- **Objective Functions**: The script uses a set of benchmark functions, including but not limited to **Rastrigin**, **Ackley**, **Griewank**, **Sphere**, and **Rosenbrock**, among others. These functions are wrapped to ensure compatibility with **IOHanalyzer** for logging.
- **Algorithms**: The algorithms evaluated include **Differential Evolution (DE)**, **CMA-ES**, **Nelder-Mead**, **PSO**, and other optimization algorithms from the Nevergrad library.
- **Budget**: The algorithms are run with a predefined budget (e.g., 5000) representing the number of function evaluations allowed.
- **Execution**: For each combination of function and algorithm, 25 runs are performed to ensure statistical robustness.
- **Logging**: The results of each run are logged into the specified directory using the **IOHanalyzer** logger for further analysis.

#### How to Run:
- The user can modify the `names` list to include the desired objective functions and the `algorithms` list to specify which algorithms to evaluate.
- The `budgets` parameter can be adjusted based on the number of function evaluations needed.
- Run the script using the following command:
  ```bash
  python exp_single_algo.py
  ```

---

### 2. `combination_auc.py`

This script processes the **results generated by single algorithms** from the `exp_single_algo.py` script and calculates the **Area Under the Curve (AUC)** for **parallel simulations** of algorithm combinations. 
It evaluates the performance of these combinations by simulating pairs of algorithms working in parallel on a suite of benchmark functions.

#### Key Functionality:
- **ECDF and AUC Calculation**: The script calculates the **Empirical Cumulative Distribution Function (ECDF)** and **AUC** values for both single algorithms and their combinations. AUC is a measure of the quality of solutions found over a range of evaluation budgets.
- **Algorithm Combinations**: The script considers all possible pairs of algorithms from the set defined in `exp_single_algo.py`, excluding identical combinations (i.e., the same algorithm twice). 
It compares the AUC values for single algorithms versus combinations to analyze the benefits of using two algorithms in parallel.
- **Data Inputs**: It reads **CSV files** containing the results of the single algorithm runs (generated using IOHanalyzer in `exp_single_algo.py`) and processes them for the AUC calculation.
- **Visualization**: It generates **heatmaps** and **bar plots** to visualize the performance improvements of combined algorithms compared to the single best solver for each function.

#### Key Functions:
- **`single_set_auc`**: Calculates the average AUC for a single algorithm based on a set of 25 runs, using target values and evaluation points.
- **`set1_set2_auc`**: Compares the results from two sets of runs of the same algorithm and calculates the better AUC from the two.
- **`ecdf_auc`**: Calculates the combined AUC for pairs of algorithms running in parallel and evaluates their performance on benchmark functions.
- **`compare_algo`**: Compares the performance of algorithm combinations against the best-performing single algorithm for each function.
- **`hmap_auc`**: Generates a heatmap to visualize the AUC values of each algorithm and combination across all functions.
- **`bar_plot`**: Creates a bar plot to rank the overall performance of individual algorithms and their combinations.

#### How to Run:
- Ensure that the **CSV files** generated by the `exp_single_algo.py` script are available for input.
- Run the script to compute the AUC for the algorithm pairs and visualize the results:
  ```bash
  python combination_auc.py
  ```

#### Output:
- **Heatmaps**: Visualize the AUC values for individual algorithms and combinations, showing which combinations perform better across functions.
- **Bar Plots**: Display the overall performance ranking of the algorithms and their combinations.
- **AUC Scores**: The calculated AUC scores are stored in dataframes and can be further analyzed for identifying the best-performing algorithm pairs.

---

### 3. `parallel2.py` (in auc_tuning_perfn or auc_tuning_overall)

This script is designed to work with **irace**, a tool for **automatic algorithm configuration**. The primary purpose of this script is to run various **optimization algorithms** (such as **CMA-ES** and **PSO**) with different hyperparameter configurations and find the **best-performing configuration** for each algorithm. The configurations are evaluated based on their performance on a suite of **benchmark functions** from the **Nevergrad** and **IOHanalyzer** libraries.

#### Key Functionality:
- **Hyperparameter Tuning**: The script uses **irace** to tune the hyperparameters of the algorithms, such as population size (`popsize_cma`, `popsize_pso`) and scaling factors, to maximize performance.
- **Supported Algorithms**: The focus is on **CMA-ES** and **PSO** algorithms, but the framework is flexible and can be extended to other algorithms supported by the **Nevergrad** library.
- **Evaluation Metric**: The performance of each configuration is evaluated using the **ECDF AUC** (Empirical Cumulative Distribution Function Area Under the Curve), which is calculated for each run based on target values and evaluation points.
- **Parameter Parsing**: The script parses command-line arguments to set up the problem and algorithm configurations and uses these to run the optimization algorithms.
- **Real-Time Performance**: During each run, the algorithms are evaluated in real time based on how well they minimize the target function values within a given budget.
  
#### Key Functions:
- **`generate_parameter_file`**: Generates an **irace-readable parameter file**, containing the hyperparameters to be tuned during the optimization process.
- **`parse_parameters`**: Parses the hyperparameters from the command line and returns two dictionaries: one for problem-specific settings and one for algorithm-specific settings.
- **`ecdf_auc`**: Calculates the **AUC** for the ECDF based on the performance of the algorithm at different evaluation points.
- **`run_target`**: Runs the **CMA-ES** and **PSO** algorithms in parallel, using the given hyperparameters to evaluate the performance on a benchmark function. It tracks the performance throughout the evaluation budget and computes the **AUC** for each run.

#### How to Run:
- Use **irace** to find the best-performing hyperparameter configuration for the specified algorithms.
- To generate the parameter file for **irace**:
  ```bash
  python parallel2.py --generate_parameters parameter_file.txt
  ```
- To run the algorithms with specific configurations:
  ```bash
  python parallel2.py configuration_id instance_name seed iid budget --fid function_id --dim dimension
  ```

#### Output:
- **AUC Scores**: The script prints the **AUC** score for each run, which measures the algorithm's performance in minimizing the objective function over the evaluation budget.
- **Hyperparameter Configurations**: The configurations that yield the best performance are recorded by **irace**, which tunes the parameters based on the feedback provided by this script.

---

### 4. `trunner.sh` (in auc_tuning_perfn or auc_tuning_overall)

This **shell script** serves as the **experiment runner** for **irace**, responsible for launching the **parallel2.py** script with the hyperparameter configurations provided by **irace**. 
The script handles the setup, execution, and result extraction from the parallel optimization experiments, allowing **irace** to efficiently explore the search space of hyperparameters and identify the best-performing configuration.

#### Key Functionality:
- **Execution of parallel2.py**: The script calls the `parallel2.py` file with the configuration parameters provided by **irace**. These parameters include the algorithm ID, instance ID, seed, and any other hyperparameters that are being tuned.
- **Handling Outputs**: After executing the **parallel2.py** script, `trunner.sh` checks the output for the objective value (cost) to be minimized. This value is extracted from the **standard output** (`stdout`) of the `parallel2.py` execution and returned to **irace**.
- **Error Handling**: The script includes robust error-checking mechanisms. It ensures that the necessary files exist and that valid numerical values are returned. 
If any error occurs during execution, the script notifies **irace** and halts further progress.
- **File Cleanup**: After retrieving the required output, the script deletes intermediate files like `

stdout` and `stderr` to maintain a clean working directory.

#### Parameters:
- **`$1`**: Configuration ID
- **`$2`**: Instance ID
- **`$3`**: Seed
- **`$4`**: Instance name
- **Additional Parameters**: Any other parameters related to the algorithmâ€™s hyperparameters that are passed from **irace**.

#### Key Steps:
1. **Setup**: The script first sets up the required paths and parameters for execution.
2. **Execution**: It then runs the **parallel2.py** script using the specified parameters. The result of the execution (objective value) is printed to the **stdout**.
3. **Output Parsing**: After execution, the script reads the result from the **stdout** file and checks if a valid numeric value was obtained.
4. **Return to irace**: The parsed objective value is printed as output and returned to **irace** for further evaluation and decision-making.
5. **File Cleanup**: Finally, it deletes the intermediate files to avoid clutter.
